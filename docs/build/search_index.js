var documenterSearchIndex = {"docs":
[{"location":"components/#Components-1","page":"Components","title":"Components","text":"","category":"section"},{"location":"components/#","page":"Components","title":"Components","text":"Different components have been defined to configure an optimizer and they are described in this page.","category":"page"},{"location":"components/#Individuals-1","page":"Components","title":"Individuals","text":"","category":"section"},{"location":"components/#","page":"Components","title":"Components","text":"An individual is a mutable structure called Metaheuristics.xf_indiv with x::Vector{Float64} and f::Float64 properties. Thus, a population can be generated as follows:","category":"page"},{"location":"components/#","page":"Components","title":"Components","text":"using Metaheuristics\n\n# your favorite objective function.\nf(x) = sum(x.^2)\n\nn, d = 100, 7\n\n# n random positions in [-10, 10]^d\nX = -10.0 .+ 200rand(n, d)\n\npopulation = [ Metaheuristics.xf_indiv( X[i,:], f(X[i,:]) ) for i in 1:n ]\n","category":"page"},{"location":"components/#","page":"Components","title":"Components","text":"Each individual in population can be deleted, edited or changed as in another mutable DataType array.","category":"page"},{"location":"components/#","page":"Components","title":"Components","text":"If you optimize the function f, the population is stored in result.population.","category":"page"},{"location":"components/#","page":"Components","title":"Components","text":"\nbounds = Array( [-10ones(d) 10ones(d)]' )\nresult = optimize(f, bounds, ECA())","category":"page"},{"location":"components/#Options-1","page":"Components","title":"Options","text":"","category":"section"},{"location":"components/#Information-1","page":"Components","title":"Information","text":"","category":"section"},{"location":"components/#","page":"Components","title":"Components","text":"Usually a global optimum is known (almost never) and maybe you want to stop the algorithm when such optimum is reached. To do that consider the following example:","category":"page"},{"location":"components/#","page":"Components","title":"Components","text":"information = Information(f_optimum = 0.0, x_optimum = zeros(d))\n\noptimizer = ECA(information = information)\nresult = optimize(f, bounds, optimizer)\n","category":"page"},{"location":"components/#Algorithm-1","page":"Components","title":"Algorithm","text":"","category":"section"},{"location":"tutorials/#Tutorials-1","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/#","page":"Tutorials","title":"Tutorials","text":"This part present different tutorials in order to exemplify how versatile Metaheuristics.jl is.","category":"page"},{"location":"tutorials/#Options-1","page":"Tutorials","title":"Options","text":"","category":"section"},{"location":"tutorials/#Comparing-Algorithms-1","page":"Tutorials","title":"Comparing Algorithms","text":"","category":"section"},{"location":"tutorials/#","page":"Tutorials","title":"Tutorials","text":"Assume that you want to optimize the following function:","category":"page"},{"location":"tutorials/#","page":"Tutorials","title":"Tutorials","text":"    f(x) = 10n + sum_i=1^n x_i^n - 10cos(2pi x_i)","category":"page"},{"location":"tutorials/#","page":"Tutorials","title":"Tutorials","text":"where x in -10 10^n and n = 10. ","category":"page"},{"location":"#Metaheuristics-1","page":"Home","title":"Metaheuristics","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A Julia package for metaheuristic optimization algorithms. Evolutionary are considered.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#Julia-0.7-or-Later-1","page":"Home","title":"Julia 0.7 or Later","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Open the Julia REPL and press ] to open the Pkg prompt. To add this package, use the add command:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"pkg> add https://github.com/jmejia8/Metaheuristics.jl.git","category":"page"},{"location":"#Quick-Start-1","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Assume that you want to minimize the function","category":"page"},{"location":"#","page":"Home","title":"Home","text":"f(x) = sum_i=1^4 x_i^2 ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"with xin -10 10^4","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using Metaheuristics\n\n# Objective function\nf(x) = sum(x.^2)\n\nbounds = [-10 -10 -10 -10;\n             10  10  10  10\n]\n\n\nresult = optimize(f, bounds, ECA())\n","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Output:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"\n+=========== STATE ==========+\n| Iter.: 1343\n| f(x) = 1.01367e-162\n| solution.x = [-4.1025820863225474e-82, 4.7085198651034774e-82, 4.991739222064757e-84, 7.897051377673628e-82]\n| f calls: 37629\n| Total time: 0.2463 s\n+============================+","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The approximation to the minimum in stored in result.best_sol, that is:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"x_optimum = result.best_sol.x\nf_x_optimum = result.best_sol.f","category":"page"},{"location":"#Optimize-1","page":"Home","title":"Optimize","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"optimize function is used to optimize a n-dimensional function","category":"page"},{"location":"#","page":"Home","title":"Home","text":"optimize(f::Function, bounds::Array, method::AbstractAlgorithm )","category":"page"},{"location":"#","page":"Home","title":"Home","text":"f objective function\nbounds a 2 times D matrix that contains the lower and upper bounds by rows.\nmethod optimization method such as ECA.","category":"page"},{"location":"algorithms/#Algorithms-1","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Different algorithms have been implemented and are referenced here.","category":"page"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"ECA algorithm\nDifferential Evolution (DE) algorithm\nParticle swarm optimization (PSO) algorithm","category":"page"},{"location":"algorithms/#ECA-1","page":"Algorithms","title":"ECA","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"ECA() is a new metaheuristic optimization algorithm based on center of mass. ECA minimizes an objective function read more..","category":"page"},{"location":"algorithms/#Parameters-1","page":"Algorithms","title":"Parameters","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"η_max: stepsize.\nK: number of neighbors for generating the center of mass.\nN: population size.","category":"page"},{"location":"algorithms/#Example-1","page":"Algorithms","title":"Example","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"using Metaheuristics\n\n# Objective function\nsphere(x) = sum(x.^2)\n\nbounds = [-10 -10 -10 -10;\n             10  10  10  10\n]\n\neca = ECA()\n\nresult = optimize(sphere, bounds, eca)\n","category":"page"},{"location":"algorithms/#DE-1","page":"Algorithms","title":"DE","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Differential Evolution DE is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. Read more...","category":"page"},{"location":"algorithms/#Parameters-2","page":"Algorithms","title":"Parameters","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"F: DE-stepsize F_weight from interval [0, 2].\nN: Number of population members.\nCR: Crossover probability constant from interval [0, 1].\nstrategy: DE strategy","category":"page"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"- `:rand1` DE/rand/1\n- `:rand2` DE/rand/2             \n- `:best1` DE/best/1             \n- `:best2` DE/best/2             \n- `:randToBest1` DE/rand-to-best/1","category":"page"},{"location":"algorithms/#Example-2","page":"Algorithms","title":"Example","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"using Metaheuristics\n\n# Objective function\nsphere(x) = sum(x.^2)\n\nbounds = [-10 -10 -10 -10;\n             10  10  10  10\n]\n\nde = DE()\n\nresult = optimize(sphere, bounds, de)\n","category":"page"},{"location":"algorithms/#PSO-1","page":"Algorithms","title":"PSO","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Particle swarm optimization is a population based stochastic optimization technique developed by Dr. Eberhart and Dr. Kennedy  in 1995, inspired by social behavior of bird flocking or fish schooling. Read more...","category":"page"},{"location":"algorithms/#Parameters-3","page":"Algorithms","title":"Parameters","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"N: Number of population members.\nC1, C2  learning factors (C1 = C2 = 2).\nω: Inertia weight used for balancing the global search.","category":"page"},{"location":"algorithms/#Example-3","page":"Algorithms","title":"Example","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"using Metaheuristics\n\n# Objective function\nsphere(x) = sum(x.^2)\n\nbounds = [-10 -10 -10 -10;\n             10  10  10  10\n]\n\npso = PSO()\n\nresult = optimize(sphere, bounds, pso)\n","category":"page"}]
}
